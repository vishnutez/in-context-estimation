{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 8.00 GiB total capacity; 19.52 GiB already allocated; 0 bytes free; 19.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 144\u001b[0m\n\u001b[1;32m    140\u001b[0m v2_ys \u001b[38;5;241m=\u001b[39m v2_task\u001b[38;5;241m.\u001b[39mevaluate(xs)\n\u001b[1;32m    141\u001b[0m v3_ys \u001b[38;5;241m=\u001b[39m v3_task\u001b[38;5;241m.\u001b[39mevaluate(xs)\n\u001b[0;32m--> 144\u001b[0m v1_pred_logits, v1_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mdemod_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv1_ys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv1_embeds = \u001b[39m\u001b[38;5;124m'\u001b[39m, v1_embeds\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#     print('Getting v1 Preds for Mixture Model')\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m#     v1_pred_logits = demod_model(xs.to(cuda_device), v1_ys.to(cuda_device)).cpu()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# np.save(f'../Files/v2_15_CE_{task}.npy', v2_CE)\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# np.save(f'../Files/v3_30_CE_{task}.npy', v3_CE)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codebase/InContextDetection/ICLDetection/CodeFiles/ModelsForInterpretability.py:294\u001b[0m, in \u001b[0;36mTransformerForICD.forward\u001b[0;34m(self, xs, ys, inds, output_hidden_states, embeds)\u001b[0m\n\u001b[1;32m    290\u001b[0m zs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine(\n\u001b[1;32m    291\u001b[0m     xs, ys\n\u001b[1;32m    292\u001b[0m )  \u001b[38;5;66;03m# interleaved x and y in n_points dim i.e. x1, y1, x2, y2, ...\u001b[39;00m\n\u001b[1;32m    293\u001b[0m embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_in(zs)\n\u001b[0;32m--> 294\u001b[0m backbone_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# if output_hidden_states=True, backbone_output.hidden_states = list of len n_layers with each element of shape (batch_size, n_points * 2, embed_dim). Each element corresponds to the output from the application of i-th layer (0th element being the input and 12th element being the output of last layer)\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# input_shape = embeds.size()[:-1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# import pdb\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    310\u001b[0m output \u001b[38;5;241m=\u001b[39m backbone_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codebase/InContextDetection/ICLDetection/CodeFiles/base_models.py:239\u001b[0m, in \u001b[0;36mGPT2ModelWOPosEncodings.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    229\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    230\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    231\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:432\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    430\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 432\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:359\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 359\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    361\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/in-context-learning/lib/python3.8/site-packages/transformers/modeling_utils.py:1871\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m   1870\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m-> 1871\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1872\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 8.00 GiB total capacity; 19.52 GiB already allocated; 0 bytes free; 19.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Based on GMMLinearRegressionResults.py.\n",
    "When running this script, the current directory should be 'src'.\n",
    "Outputs will be saved to 'src/plots'.\n",
    "\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
    "from samplers import get_data_sampler\n",
    "from tasks import get_task_sampler, SparseLinearRegression\n",
    "from tasks import LinearRegression as LinearRegressionTask\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, SGDRegressor, Ridge\n",
    "import numpy as np\n",
    "from munch import Munch\n",
    "import yaml\n",
    "import ModelsForInterpretability\n",
    "\n",
    "\n",
    "def get_model_from_run(run_path, step=-1, only_conf=False):\n",
    "    config_path = os.path.join(run_path, \"config.yaml\")\n",
    "    with open(config_path) as fp:  # we don't Quinfig it to avoid inherits\n",
    "        conf = Munch.fromDict(yaml.safe_load(fp))\n",
    "    if only_conf:\n",
    "        return None, conf\n",
    "\n",
    "    model = ModelsForInterpretability.build_model(conf.model)\n",
    "\n",
    "    if step == -1:\n",
    "        state_path = os.path.join(run_path, \"state.pt\")\n",
    "        state = torch.load(state_path)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "    else:\n",
    "        model_path = os.path.join(run_path, f\"model_{step}.pt\")\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model, conf\n",
    "\n",
    "\n",
    "sns.set_theme(\"notebook\", \"darkgrid\")\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 300\n",
    "mpl.rcParams[\"text.usetex\"] = False\n",
    "\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"axes.titlesize\": 8,\n",
    "        \"figure.titlesize\": 10,  # was 10\n",
    "        \"legend.fontsize\": 12,  # was 10\n",
    "        \"xtick.labelsize\": 6,\n",
    "        \"ytick.labelsize\": 6,\n",
    "    }\n",
    ")\n",
    "run_dir = \"../models/\"\n",
    "\n",
    "SPINE_COLOR = \"gray\"\n",
    "\n",
    "\n",
    "def format_axes(ax):\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_color(SPINE_COLOR)\n",
    "        ax.spines[spine].set_linewidth(0.5)\n",
    "\n",
    "    for spine in [\"left\", \"bottom\"]:\n",
    "        ax.spines[spine].set_color(SPINE_COLOR)\n",
    "        ax.spines[spine].set_linewidth(0.5)\n",
    "\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "    for axis in [ax.xaxis, ax.yaxis]:\n",
    "        axis.set_tick_params(direction=\"out\", color=SPINE_COLOR)\n",
    "    return ax\n",
    "\n",
    "cuda_device = \"cuda:0\"\n",
    "\n",
    "task = \"tf_detection_time_variant_snr_0\"\n",
    "run_id = \"model_1\"\n",
    "\n",
    "\n",
    "# Only used for --save_predictions and --load_predictions\n",
    "params_dict_file = os.path.join('plots', f'params_dict_{task}_{run_id}.pickle')\n",
    "v1_errors_file = os.path.join('plots', f'v1_errors_{task}_{run_id}.npy')\n",
    "v2_errors_file = os.path.join('plots', f'v2_errors_{task}_{run_id}.npy')\n",
    "v3_errors_file = os.path.join('plots', f'v3_errors_{task}_{run_id}.npy')\n",
    "\n",
    "batch_size = 10000  # 1280 #conf.training.batch_size\n",
    "n_dims = 8\n",
    "seed = 45\n",
    "\n",
    "demod_model, demod_conf = get_model_from_run(os.path.join(run_dir, task, run_id))\n",
    "demod_model.to(cuda_device)\n",
    "\n",
    "n_points = demod_conf.training.curriculum.points.end\n",
    "data_sampler = get_data_sampler(demod_conf.training.data, n_dims)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Create 3 tasks, for the three tasks in the mixture.\n",
    "v1_task_kwargs = copy.deepcopy(demod_conf.training.task_kwargs)\n",
    "v1_task_kwargs.v_probs = [1.0, 0.0, 0.0]\n",
    "v2_task_kwargs = copy.deepcopy(demod_conf.training.task_kwargs)\n",
    "v2_task_kwargs.v_probs = [0.0, 1.0, 0.0]\n",
    "v3_task_kwargs = copy.deepcopy(demod_conf.training.task_kwargs)\n",
    "v3_task_kwargs.v_probs = [0.0, 0.0, 1.0]\n",
    "\n",
    "v1_task = get_task_sampler(\n",
    "    demod_conf.training.task, n_dims, batch_size, **v1_task_kwargs\n",
    ")()\n",
    "\n",
    "v2_task = get_task_sampler(\n",
    "    demod_conf.training.task, n_dims, batch_size, **v2_task_kwargs\n",
    ")()\n",
    "\n",
    "v3_task = get_task_sampler(\n",
    "    demod_conf.training.task, n_dims, batch_size, **v3_task_kwargs\n",
    ")()\n",
    "\n",
    "xs, sig_ids = data_sampler.sample_xs(b_size=batch_size, n_points=n_points)\n",
    "\n",
    "# Compute ys under the context v1 = 5 m/s\n",
    "v1_ys = v1_task.evaluate(xs)\n",
    "v2_ys = v2_task.evaluate(xs)\n",
    "v3_ys = v3_task.evaluate(xs)\n",
    "\n",
    "\n",
    "v1_pred_logits, v1_embeds = demod_model(xs.to(cuda_device), v1_ys.to(cuda_device)).cpu()\n",
    "\n",
    "\n",
    "print('v1_embeds = ', v1_embeds.shape)\n",
    "\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     print('Getting v1 Preds for Mixture Model')\n",
    "#     v1_pred_logits = demod_model(xs.to(cuda_device), v1_ys.to(cuda_device)).cpu()\n",
    "#     print('Getting v2 Preds for Mixture Model')\n",
    "#     v2_pred_logits = demod_model(xs.to(cuda_device), v2_ys.to(cuda_device)).cpu()\n",
    "#     print('Getting v3 Preds for Mixture Model')\n",
    "#     v3_pred_logits = demod_model(xs.to(cuda_device), v3_ys.to(cuda_device)).cpu()\n",
    "\n",
    "#     v1_pred_sig_ids = torch.argmax(v1_pred_logits, dim=-1).detach().numpy()\n",
    "#     v2_pred_sig_ids = torch.argmax(v2_pred_logits, dim=-1).detach().numpy()\n",
    "#     v3_pred_sig_ids = torch.argmax(v3_pred_logits, dim=-1).detach().numpy()\n",
    "\n",
    "# metric = v1_task.get_metric()\n",
    "\n",
    "\n",
    "# v1_post = torch.softmax(v1_pred_logits, dim=-1).detach().numpy()\n",
    "# v2_post = torch.softmax(v2_pred_logits, dim=-1).detach().numpy()\n",
    "# v3_post = torch.softmax(v3_pred_logits, dim=-1).detach().numpy()\n",
    "\n",
    "# s_ids_np = sig_ids.detach().numpy()\n",
    "\n",
    "# batch_size, cnxt_len = s_ids_np.shape\n",
    "# B_indices = np.arange(batch_size)[:, None]\n",
    "\n",
    "# # Compute cross entropy (CE)\n",
    "# v1_CE = -np.log(v1_post[B_indices, np.arange(cnxt_len), s_ids_np])  \n",
    "# v2_CE = -np.log(v2_post[B_indices, np.arange(cnxt_len), s_ids_np])\n",
    "# v3_CE = -np.log(v3_post[B_indices, np.arange(cnxt_len), s_ids_np])\n",
    "\n",
    "# print('v1_CE = ', v1_CE.shape)\n",
    "\n",
    "\n",
    "# def get_df_from_pred_array(pred_arr, n_points, offset=0):\n",
    "#     # pred_arr --> b x pts-1\n",
    "#     batch_size = pred_arr.shape[0]\n",
    "#     flattened_arr = pred_arr.ravel()\n",
    "#     points = np.array(list(range(offset, n_points)) * batch_size)\n",
    "#     df = pd.DataFrame({\"y\": flattened_arr, \"x\": points})\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def lineplot_with_ci(pred_or_err_arr, n_points, offset, label, ax, seed):\n",
    "#     sns.lineplot(\n",
    "#         data=get_df_from_pred_array(pred_or_err_arr, n_points=n_points, offset=offset),\n",
    "#         y=\"y\",\n",
    "#         x=\"x\",\n",
    "#         label=label,\n",
    "#         ax=ax,\n",
    "#         n_boot=1000,\n",
    "#         seed=seed,\n",
    "#         ci=90,\n",
    "#         color=blue,\n",
    "#         marker=m1,\n",
    "#         linewidth=5,\n",
    "#         markersize=22,\n",
    "#     )\n",
    "\n",
    "\n",
    "# sns.set(style=\"whitegrid\", font_scale=2.5)\n",
    "# # latexify(4, 3)\n",
    "\n",
    "# blue = \"#377eb8\"\n",
    "# orange = \"#ff7f00\"\n",
    "# green = \"#4daf4a\"\n",
    "# m1 = \"o\"\n",
    "# m2 = \"X\"\n",
    "# m3 = \"^\"\n",
    "\n",
    "# lr_bound = n_dims\n",
    "# # Correct aspect ratio to use: width 4.8 to height 1.5.\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(48, 15), constrained_layout=True, sharey='all')\n",
    "\n",
    "# lineplot_with_ci(\n",
    "#     v1_CE,\n",
    "#     n_points,\n",
    "#     offset=0,\n",
    "#     label=\"TF\",\n",
    "#     ax=ax1,\n",
    "#     seed=seed,\n",
    "# )\n",
    "\n",
    "# ax1.set_xlabel(\"Context length\")\n",
    "# ax1.set_ylabel(\"SEP\")\n",
    "# ax1.set_title(\"Velocity = 5 m/s\")\n",
    "\n",
    "# format_axes(ax1)\n",
    "\n",
    "# lineplot_with_ci(\n",
    "#     v2_CE,\n",
    "#     n_points,\n",
    "#     offset=0,\n",
    "#     label=\"TF\",\n",
    "#     ax=ax2,\n",
    "#     seed=seed,\n",
    "# )\n",
    "\n",
    "# ax2.set_xlabel(\"Context length\")\n",
    "# ax2.set_ylabel(\"SEP\")\n",
    "# ax2.set_title(\"Velocity = 15 m/s\")\n",
    "\n",
    "# format_axes(ax2)\n",
    "\n",
    "# lineplot_with_ci(\n",
    "#     v3_CE,\n",
    "#     n_points,\n",
    "#     offset=0,\n",
    "#     label=\"TF\",\n",
    "#     ax=ax3,\n",
    "#     seed=seed,\n",
    "# )\n",
    "\n",
    "# ax3.set_xlabel(\"Context length\")\n",
    "# ax3.set_ylabel(\"SEP\")\n",
    "# ax3.set_title(\"Velocity = 30 m/s\")\n",
    "\n",
    "# format_axes(ax3)\n",
    "\n",
    "# leg = ax1.legend(loc=\"upper right\")\n",
    "# leg = ax2.legend(loc=\"upper right\")\n",
    "# leg = ax3.legend(loc=\"upper right\")\n",
    "\n",
    "# for line in leg.get_lines():\n",
    "#     line.set_linewidth(5)\n",
    "\n",
    "# plt.savefig(f\"../Plots/{args.task_name}_{args.run_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# np.save(f'../Files/v1_5_CE_{task}.npy', v1_CE)\n",
    "# np.save(f'../Files/v2_15_CE_{task}.npy', v2_CE)\n",
    "# np.save(f'../Files/v3_30_CE_{task}.npy', v3_CE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in-context-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
